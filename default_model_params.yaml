experiment_params:
  num_seeds: 20
  num_episodes: 20
  trials_per_episode: 40
  agents_per_condition: 50
  hrl_exp_save_path: logs/hrl

healthy:
  policy_params:
    learning_rate: 0.1      # moderate learning speed
    gamma: 0.95             # future rewards matter a lot
    epsilon: 0.8            # biased towards exploration, but still some exploitation
    temperature: 1.0        # standard randomness in action selection
  dopamine_modulation:
    scaling: 1.0            # normal RPE signal
    sensitivity: 1.0        # equal weighting of rewards and punishments

overactive:
  policy_params:
    learning_rate: 0.2      # faster learning
    gamma: 0.1              # much more inclined to value immediate rewards
    epsilon: 0.1            # much greedier exploitation (less exploration)
    temperature: 0.5        # much less randomness in action selection
  dopamine_modulation:
    scaling: 2.0            # exaggerated RPE signal
    sensitivity: 0.5        # punishments down-weighted

depleted:
  policy_params:
    learning_rate: 0.05     # slower learning
    gamma: 1.0              # completely future-oriented
    epsilon: 1.0            # completely random (exploration), no exploitation
    temperature: 2.0        # much more randomness in action selection
  dopamine_modulation:
    scaling: 0.5            # weaker RPE signal
    sensitivity: 2.0        # punishments hit harder
